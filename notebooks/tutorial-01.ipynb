{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain QuickStart Tutorial\n",
    "\n",
    "I'm essentially following this QuickStart:\n",
    "\n",
    "https://python.langchain.com/docs/get_started/quickstart\n",
    "\n",
    "However, I've modified some of the statements and queries. I'm also using Azure OpenAI so some of the imports differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "\n",
    "# Use a local .env file to populate environment variables\n",
    "dotenv.load_dotenv(override=True)\n",
    "\n",
    "model_deployment = os.getenv('MODEL_DEPLOYMENT')\n",
    "embedding_deployment = os.getenv('EMBEDDING_DEPLOYMENT')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first cell creates a basic chain with a system prompt, the llm, and an output parser.\n",
    "It then asks the LLM \"Who is Martin Peck?\" and prints the response.\n",
    "It should give a response similar to the following:\n",
    "\n",
    "> I'm sorry, but as a software engineer, I don't have specific information on individuals unless they are widely recognized in the field of software engineering or technology. Could you please provide more context?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but as a software engineer, I don't have specific information on individuals unless they are widely recognized in the field of technology or software engineering. Martin Peck doesn't appear to be a notable figure in these areas based on the information available to me.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = AzureChatOpenAI(deployment_name=model_deployment)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a software engineer. When responding, be brief and to the point.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "chain.invoke({\"input\": \"Who is Martin Peck?\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell uses the WebBaseLoader (which, under the covers, uses BeautifulSoup) to scrape my blog \"About\" page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# scrape the web page\n",
    "loader = WebBaseLoader(\"https://martinpeck.com/about/\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we prepare an embedding model (again, using the Azure OpenAI service), chunk the scraped web page, and then generate and store embeddings in a local FAISS vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# prep the embedding model\n",
    "embeddings = AzureOpenAIEmbeddings(azure_deployment=embedding_deployment)\n",
    "\n",
    "# split the scraped web page into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# create embeddings and store them in a local FAISS vector store\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we set up a chat template that allows us to inject both context and a question into the chat.\n",
    "When we invoke this chain the LLM is given the context from the vector store, and this time when asked \"Who is Martin Peck\" should respond with something similar to the following:\n",
    "\n",
    "> Martin Peck is a software engineer with over 20 years of experience in writing software and leading software development teams. He is currently a Software Engineering Manager at Microsoft, where he manages a team of Software Engineers. He has also worked for blinkbox and was previously the Director of Engineering for a not-for-profit called Code Club. In addition to his work in software, he also draws and publishes cartoons, some of which can be found for sale at Redbubble and Zazzle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Martin Peck is a software developer and team leader with over 20 years of experience. He has worked for Microsoft, where he is currently a Software Engineering Manager, and for a not-for-profit organization called Code Club, where he was the Director of Engineering. He also worked for blinkbox in various roles. In addition to his professional roles, Martin Peck is a volunteer who runs a coding club at a local school. He also draws and publishes cartoons.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "response = retrieval_chain.invoke({\"input\": \"Who is Martin Peck?\"})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
